{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "14.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJIQ0GquW6Qp",
        "outputId": "9112a0b6-8fe2-41d9-fc8e-192041d2c30e"
      },
      "source": [
        "pip install tensorflow==1.14"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 54kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 35.3MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (51.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg383brPW6pv",
        "outputId": "2f9e78ea-e5a4-4ecf-da0a-48ab064a8316"
      },
      "source": [
        "pip install plotly==3.10.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting plotly==3.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/75/3982bac5076d0ce6d23103c03840fcaec90c533409f9d82c19f54512a38a/plotly-3.10.0-py2.py3-none-any.whl (41.5MB)\n",
            "\u001b[K     |████████████████████████████████| 41.5MB 108kB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly==3.10.0) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from plotly==3.10.0) (2.23.0)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly==3.10.0) (5.0.8)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==3.10.0) (1.3.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from plotly==3.10.0) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==3.10.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->plotly==3.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->plotly==3.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->plotly==3.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->plotly==3.10.0) (2020.12.5)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==3.10.0) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==3.10.0) (4.7.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==3.10.0) (4.3.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==3.10.0) (0.2.0)\n",
            "\u001b[31mERROR: cufflinks 0.17.3 has requirement plotly>=4.1.1, but you'll have plotly 3.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: plotly\n",
            "  Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "Successfully installed plotly-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "zmzoqTF2WxG7",
        "outputId": "bc4f6488-c9a8-439c-a2aa-1de8ebb756be"
      },
      "source": [
        "# 필요한 패키지 Import\n",
        "import tensorflow as tf  \n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata  # Unicode Character Database를 접근할 수 있도록 하는 모듈\n",
        "import re          # regular expression package\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import string\n",
        "\n",
        "import plotly  # plotly는 3.10.0 버전을 사용함, Plotly Python Open Source Graphing Library\n",
        "import plotly.plotly as py\n",
        "\n",
        "from plotly.offline import init_notebook_mode, iplot \n",
        "\n",
        "plotly.offline.init_notebook_mode(connected=True)\n",
        "\n",
        "\n",
        "import plotly.graph_objs as go"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "lNHk7P1CWxG8",
        "outputId": "3b51322b-bad5-4201-a7af-d656b10816e7"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SaKDi56fWxG9",
        "outputId": "de0e5eb9-f332-4433-cfa8-7ff47fa144de"
      },
      "source": [
        "plotly.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.10.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5kfcFyIY2tX",
        "outputId": "06a28313-25ba-4ee8-e099-d34bb02bbf89"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHS-4zH6WxG-"
      },
      "source": [
        "file_path = '/content/gdrive/My Drive/Colab Notebooks/kor-eng/kor.txt'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhzEwBDWWxG_",
        "outputId": "1dc701c3-9085-4ef5-f06b-8450c3375046"
      },
      "source": [
        "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n') \n",
        "# 각 라인을 \\t로 분리했을 때 처음 2개 string만 추출 \n",
        "lines = [l.split('\\t')[:2] for l in lines]\n",
        "# 영문장과 해당 번역문을 \\t로 묶어 하나의 라인 형성\n",
        "lines = [l[0] + '\\t' + l[1] for l in lines]\n",
        "# 200번째 라인부터 10개 라인 추출하여 출력\n",
        "lines[200:210] "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Some water, please.\\t물 좀 주세요, 제발.',\n",
              " 'The light went out.\\t전등이 꺼졌다.',\n",
              " 'The night was cold.\\t그날 밤은 추웠어.',\n",
              " 'Tie your shoelaces.\\t신발끈을 묶으세요.',\n",
              " 'We were retreating.\\t우리는 후퇴하고 있었다.',\n",
              " \"We've been worried.\\t계속 걱정했어.\",\n",
              " 'When will you come?\\t언제쯤 올거야?',\n",
              " 'Whose book is this?\\t이것은 누구의 책입니까?',\n",
              " 'Can I have this cup?\\t이 컵 가져도 돼요?',\n",
              " 'Do you like English?\\t영어 좋아해요?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnugPEqVWxG_",
        "outputId": "12d3dc5b-4e8b-4adc-e1c5-52d275465616"
      },
      "source": [
        "len(lines)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWJHyZI-WxHA"
      },
      "source": [
        "exclude = set(string.punctuation) # 모든 특수 문자를 exclude 변수에 저장\n",
        "remove_digits = str.maketrans('', '', string.digits) # 숫자로 구성된 Set을 remove_digits에 저장"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUHdH58kWxHA"
      },
      "source": [
        "# 영어 문장을 전처리\n",
        "def preprocess_eng_sentence(sent):\n",
        "    sent = sent.lower() # 영어 문자를 소문자로\n",
        "    sent = re.sub(\"'\", '', sent) # \"'\"부호가 있으면 삭제\n",
        "    sent = ''.join(ch for ch in sent if ch not in exclude) # 특수부호를 삭제\n",
        "    sent = sent.translate(remove_digits) # 모든 숫자를 제거\n",
        "    sent = sent.strip() # 문자열의 양 끝에 존재하는 공백과 \\n 제거\n",
        "    sent = re.sub(\" +\", \" \", sent) # 특정 패턴의 문자열(\" +\")을 다른 문자열(\" \")로 바꾸기 \n",
        "    sent = '<start> ' + sent + ' <end>' # <start> 와 <end> token을 추가\n",
        "    return sent"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsc9-iOfWxHB"
      },
      "source": [
        "# 한글 문장을 전처리\n",
        "def preprocess_kor_sentence(sent):\n",
        "    sent = re.sub(\"'\", '', sent) # \"'\"부호가 있으면 삭제\n",
        "    sent = ''.join(ch for ch in sent if ch not in exclude) # 특수부호를 삭제\n",
        "    sent = sent.strip() # 문자열의 양 끝에 존재하는 공백과 \\n 제거\n",
        "    sent = re.sub(\" +\", \" \", sent) # 특정 패턴의 문자열(\" +\")을 다른 문자열(\" \")로 바꾸기\n",
        "    sent = '<start> ' + sent + ' <end>' # <start> 와 <end> token을 추가\n",
        "    return sent"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dCrtlSkWxHC",
        "outputId": "a37f1ff7-c224-40ee-d92c-0c9d189f4bac"
      },
      "source": [
        "# 정제된 영어와 한국어 문장을 쌍으로 생성\n",
        "sent_pairs = []\n",
        "for line in lines:\n",
        "    sent_pair = []\n",
        "    eng, mar = line.split('\\t') # 영문장 및 번역문 쌍을 각각 eng와 mar로 나누어 저장 \n",
        "    \n",
        "    # 영어 문장\n",
        "    eng = preprocess_eng_sentence(eng) # 영문장 eng 전처리\n",
        "    sent_pair.append(eng) # 전처리 된 영문장을 sent_pair 리스트의 첫번째 원소로 저장\n",
        "    # 한글 문장\n",
        "    mar = preprocess_kor_sentence(mar) # 번역문 mar 전처리\n",
        "    sent_pair.append(mar) # 전처리 된 번역문을 sent_pair 리스트의 두번째 원소로 저장\n",
        "    sent_pairs.append(sent_pair) # 전처리가 끝난 영문장과 번역문으로 이루어진 리스트를 sent_pairs 리스트에 저장\n",
        "\n",
        "# 200번째 라인부터 10개 라인 추출\n",
        "sent_pairs[200:210]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start> some water please <end>', '<start> 물 좀 주세요 제발 <end>'],\n",
              " ['<start> the light went out <end>', '<start> 전등이 꺼졌다 <end>'],\n",
              " ['<start> the night was cold <end>', '<start> 그날 밤은 추웠어 <end>'],\n",
              " ['<start> tie your shoelaces <end>', '<start> 신발끈을 묶으세요 <end>'],\n",
              " ['<start> we were retreating <end>', '<start> 우리는 후퇴하고 있었다 <end>'],\n",
              " ['<start> weve been worried <end>', '<start> 계속 걱정했어 <end>'],\n",
              " ['<start> when will you come <end>', '<start> 언제쯤 올거야 <end>'],\n",
              " ['<start> whose book is this <end>', '<start> 이것은 누구의 책입니까 <end>'],\n",
              " ['<start> can i have this cup <end>', '<start> 이 컵 가져도 돼요 <end>'],\n",
              " ['<start> do you like english <end>', '<start> 영어 좋아해요 <end>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIjOYcW7WxHD"
      },
      "source": [
        "# 단어에서 인텍스의 매핑과 인텍스에서 단어의 매핑을 생성하는 클래스 정의\n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        # 변수 및 형태(타입) 선언\n",
        "        self.lang = lang # lang \n",
        "        self.word2idx = {} # 단어-인덱스 : 딕셔너리 타입\n",
        "        self.idx2word = {} # 인덱스-단어 : 딕셔너리 타입\n",
        "        self.vocab = set() # 사전 구축(데이터의 중복을 허용하지 않으며 저장되는 데이터에 대한 순서가 없음) \n",
        "\n",
        "        self.create_index() # 인덱스 생성 함수 선언\n",
        "    \n",
        "    # 단어의 Index를 생성\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' ')) # 띄어쓰기(' ')로 나뉘어 생성된 단어(어절)를 사전에 추가\n",
        "\n",
        "        self.vocab = sorted(self.vocab) # 알파벳 순서대로 사전(단어) 정렬\n",
        "\n",
        "        self.word2idx['<pad>'] = 0 # <pad> 단어에 인덱스 0을 지정 (예시: word2idx = {'<pad>': 0})\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # 위에서 생성된 사전(vocab)의 각 단어에 인덱스 생성(1부터)\n",
        "\n",
        "        for word, index in self.word2idx.items():\n",
        "            # word2idx의 각 단어에 지정된 인덱스를 idx2word에 인덱스: 단어 형식으로 지정\n",
        "            # 예시: idx2word = {0:'<pad>'}\n",
        "            self.idx2word[index] = word "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wieAVd2WxHE"
      },
      "source": [
        "# 텐서의 최대 길이를 계산하는 함수\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SwnygNzWxHF"
      },
      "source": [
        "# 입력 데이터와 타겟 데이터에 대해 Indexing 작업을 진행\n",
        "# 패딩(Padding) 작업 진행\n",
        "def load_dataset(pairs, num_examples):\n",
        "    # pairs => already created cleaned input, output pairs\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
        "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
        "    \n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
        "    \n",
        "    # 한국어 문장\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
        "    \n",
        "    # 입력 텐서(영어 문장), 출력 텐서(한국어 문장)에서 최대 길이를 각각 계산함 \n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, # 영어문장\n",
        "                                                                 maxlen=max_length_inp, # 영어문장 텐서의 최대 길이 \n",
        "                                                                 padding='post') # 'pre' 또는 'post': pre를 쓰면 시퀀스 앞에 패딩을 하고 post를 쓰면 시퀀스 뒤에 패딩\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, # 번역문\n",
        "                                                                  maxlen=max_length_tar, # 번역문 텐서의 최대 길이 \n",
        "                                                                  padding='post') # 'pre' 또는 'post': pre를 쓰면 시퀀스 앞에 패딩을 하고 post를 쓰면 시퀀스 뒤에 패딩\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZsR16K2WxHF",
        "outputId": "ed0b7a31-5cd1-4a44-ae79-f4f62e62da03"
      },
      "source": [
        "# load_dataset() 함수 호출\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))\n",
        "\n",
        "print(input_tensor)\n",
        "print(\"The max length of Input Tensor: \", max_length_inp)\n",
        "print(\"The max length of Output Tensor: \", max_length_targ)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   2 1099    1 ...    0    0    0]\n",
            " [   2  436    1 ...    0    0    0]\n",
            " [   2  656 1079 ...    0    0    0]\n",
            " ...\n",
            " [   2 1002  665 ...    0    0    0]\n",
            " [   2  468  981 ...  997  822    1]\n",
            " [   2  497  660 ... 1129    1    0]]\n",
            "The max length of Input Tensor:  19\n",
            "The max length of Output Tensor:  17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLvhR7J2WxHG",
        "outputId": "ecd69b13-92aa-4bc1-c867-3b1228da9f3b"
      },
      "source": [
        "# 훈련용 데이터와 테스트 데이터를 9대1로 분할\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, \\\n",
        "                                                                                                target_tensor, test_size=0.1\n",
        "                                                                                             , random_state = 101)\n",
        "\n",
        "# 분할된 데이터 개수를 보여줌\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(813, 813, 91, 91)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2KToA2zWxHH",
        "outputId": "13c03e0a-cc10-45bf-c69b-1ce80ae0372a"
      },
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "BUFFER_SIZE = len(input_tensor_train) # 학습데이터 사이즈\n",
        "BATCH_SIZE = 64 # batch 사이즈\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE # batch 수\n",
        "embedding_dim = 256 # 임베딩 차원\n",
        "units = 1024 # 신경망 유닛 수\n",
        "\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "print(dataset)\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) \n",
        "print(dataset)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((19,), (17,)), types: (tf.int32, tf.int32)>\n",
            "<DatasetV1Adapter shapes: ((64, 19), (64, 17)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAwao3hKWxHH"
      },
      "source": [
        "def gru(units):\n",
        "    return tf.keras.layers.GRU(units, # 결과값 차원 수(dimensionality of the output space)\n",
        "                               return_sequences=True, # 마지막 시퀀스를 출력할 것인지, 아니면 전체 시퀀스를 출력할 것인지 여부\n",
        "                               return_state=True, # output 외에도 최후 state를 출력할 것인지 여부\n",
        "                               recurrent_activation='sigmoid', # recurrent 단계에서 activation function\n",
        "                               recurrent_initializer='glorot_uniform') # recurrent_kernel 가중치 행렬 초기화"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG11HQalWxHI"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz # 배치 사이즈\n",
        "        self.enc_units = enc_units # 유닛(unit) 개수\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 임베딩 층, (영어 단어의 개수, 임베딩 차원)\n",
        "        self.gru = gru(self.enc_units) # GRU 층\n",
        "    \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    # Hidden state를 초기화\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b88RkoXWWxHK"
      },
      "source": [
        "# 디코더 클래스를 정의\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz # 배치 사이즈\n",
        "        self.dec_units = dec_units # 유닛(unit) 개수\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 임베딩 층\n",
        "        self.gru = gru(self.dec_units) # GRU 층\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size) # 전체 한국어 단어의 개수\n",
        "        \n",
        "        # Attention에서 사용함\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units) # encoder ouptput 가중치\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units) # RNN 셀 각각의 output을 입력값으로 취하는 Feed-Forward Fully Connected Layer\n",
        "        self.V = tf.keras.layers.Dense(1) # context vector\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usFWYq2_WxHL"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE) "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f96r10AWxHM"
      },
      "source": [
        "# 최적화 함수\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# 손실함수 정의\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0) # 원소 단위로 비교 연산을 만족하면 True 반환, 만족하지 않으면 False 반환함\n",
        "    # softmax 산출 후 cross_entropy를 구하는 함수\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlCFuVQ0WxHN"
      },
      "source": [
        "# 체크 포인트 생성\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuR8aj-nWxHN",
        "outputId": "d6acf0ce-1c5d-4bec-efe6-c266efe3d810"
      },
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "# Epoch 만큼 반복해 모델을 훈련함\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # 인코더의 히든 상태를 초기화\n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    # 전체 손실을 저장하는 변수를 정의하고 0을 초기값으로 설정\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            # 디코더의 입력을 준비 (디코더 모델의 첫번째 스텝의 입력을 '<start>'의 index 번호로 설정함)\n",
        "            # shape은 (64, 1)\n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)    \n",
        "            \n",
        "            for t in range(1, targ.shape[1]):\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        # 배치 손실을 계산\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        # 전체 손실의 누적 합\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        # 업데이트를 하기 위한 인코더와 디코더의 가중치를 가지고 옴\n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        # 손실을 사용해 기울기 값을 계산함\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        # 기울기 값을 사용해 가중치를 업데이트함\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        # 100번째 배치마다 출력\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # 매 Epoch마다 모델을 체크포인트 폴더에 저장\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.3582\n",
            "Epoch 1 Loss 2.2633\n",
            "Time taken for 1 epoch 119.50345182418823 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.2094\n",
            "Epoch 2 Loss 1.9803\n",
            "Time taken for 1 epoch 116.58539628982544 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.0364\n",
            "Epoch 3 Loss 1.8828\n",
            "Time taken for 1 epoch 114.25469326972961 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.9310\n",
            "Epoch 4 Loss 1.7947\n",
            "Time taken for 1 epoch 115.58210253715515 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.7947\n",
            "Epoch 5 Loss 1.7108\n",
            "Time taken for 1 epoch 116.35735630989075 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.7046\n",
            "Epoch 6 Loss 1.6378\n",
            "Time taken for 1 epoch 116.09512782096863 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.6448\n",
            "Epoch 7 Loss 1.5860\n",
            "Time taken for 1 epoch 116.10777235031128 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.6055\n",
            "Epoch 8 Loss 1.5466\n",
            "Time taken for 1 epoch 117.68636584281921 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.5630\n",
            "Epoch 9 Loss 1.5080\n",
            "Time taken for 1 epoch 115.96210074424744 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.5157\n",
            "Epoch 10 Loss 1.4671\n",
            "Time taken for 1 epoch 114.62306213378906 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.4690\n",
            "Epoch 11 Loss 1.4239\n",
            "Time taken for 1 epoch 116.08216738700867 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.4069\n",
            "Epoch 12 Loss 1.3716\n",
            "Time taken for 1 epoch 115.16805386543274 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.3581\n",
            "Epoch 13 Loss 1.3204\n",
            "Time taken for 1 epoch 115.40487790107727 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.2969\n",
            "Epoch 14 Loss 1.2697\n",
            "Time taken for 1 epoch 116.93703293800354 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.2524\n",
            "Epoch 15 Loss 1.2302\n",
            "Time taken for 1 epoch 115.20226860046387 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.1703\n",
            "Epoch 16 Loss 1.2139\n",
            "Time taken for 1 epoch 117.11297082901001 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.2083\n",
            "Epoch 17 Loss 1.2057\n",
            "Time taken for 1 epoch 116.24318170547485 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.1674\n",
            "Epoch 18 Loss 1.1496\n",
            "Time taken for 1 epoch 115.12561440467834 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.1413\n",
            "Epoch 19 Loss 1.0752\n",
            "Time taken for 1 epoch 116.68908858299255 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0433\n",
            "Epoch 20 Loss 1.0190\n",
            "Time taken for 1 epoch 115.63931512832642 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.9732\n",
            "Epoch 21 Loss 0.9634\n",
            "Time taken for 1 epoch 114.69257688522339 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.9711\n",
            "Epoch 22 Loss 1.0357\n",
            "Time taken for 1 epoch 117.59370994567871 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.9812\n",
            "Epoch 23 Loss 0.9552\n",
            "Time taken for 1 epoch 115.06313014030457 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.9113\n",
            "Epoch 24 Loss 0.8561\n",
            "Time taken for 1 epoch 115.50794243812561 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.7859\n",
            "Epoch 25 Loss 0.7669\n",
            "Time taken for 1 epoch 117.48985719680786 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.6925\n",
            "Epoch 26 Loss 0.6815\n",
            "Time taken for 1 epoch 115.42800283432007 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.6337\n",
            "Epoch 27 Loss 0.6248\n",
            "Time taken for 1 epoch 116.63510918617249 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.5808\n",
            "Epoch 28 Loss 0.5806\n",
            "Time taken for 1 epoch 116.38653445243835 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.5564\n",
            "Epoch 29 Loss 0.5439\n",
            "Time taken for 1 epoch 115.91938328742981 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.5127\n",
            "Epoch 30 Loss 0.5229\n",
            "Time taken for 1 epoch 119.71693849563599 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.5077\n",
            "Epoch 31 Loss 0.4959\n",
            "Time taken for 1 epoch 114.6609251499176 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.4781\n",
            "Epoch 32 Loss 0.4854\n",
            "Time taken for 1 epoch 115.56106567382812 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.4552\n",
            "Epoch 33 Loss 0.4585\n",
            "Time taken for 1 epoch 120.32868814468384 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.4478\n",
            "Epoch 34 Loss 0.4444\n",
            "Time taken for 1 epoch 115.49793982505798 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.4226\n",
            "Epoch 35 Loss 0.4222\n",
            "Time taken for 1 epoch 115.42500019073486 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.4100\n",
            "Epoch 36 Loss 0.4091\n",
            "Time taken for 1 epoch 118.11295247077942 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.3922\n",
            "Epoch 37 Loss 0.3979\n",
            "Time taken for 1 epoch 114.98676490783691 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.3916\n",
            "Epoch 38 Loss 0.3874\n",
            "Time taken for 1 epoch 116.40135169029236 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.3743\n",
            "Epoch 39 Loss 0.3834\n",
            "Time taken for 1 epoch 116.36486554145813 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.3943\n",
            "Epoch 40 Loss 0.3766\n",
            "Time taken for 1 epoch 115.15861248970032 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.3635\n",
            "Epoch 41 Loss 0.3673\n",
            "Time taken for 1 epoch 118.53313541412354 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.3749\n",
            "Epoch 42 Loss 0.3698\n",
            "Time taken for 1 epoch 115.50950622558594 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.3674\n",
            "Epoch 43 Loss 0.3695\n",
            "Time taken for 1 epoch 115.7361102104187 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.3350\n",
            "Epoch 44 Loss 0.3613\n",
            "Time taken for 1 epoch 118.86955761909485 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3823\n",
            "Epoch 45 Loss 0.3555\n",
            "Time taken for 1 epoch 116.16027760505676 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.3309\n",
            "Epoch 46 Loss 0.3338\n",
            "Time taken for 1 epoch 116.9327802658081 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.3081\n",
            "Epoch 47 Loss 0.3195\n",
            "Time taken for 1 epoch 117.744619846344 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.3100\n",
            "Epoch 48 Loss 0.3114\n",
            "Time taken for 1 epoch 115.59679460525513 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3129\n",
            "Epoch 49 Loss 0.3095\n",
            "Time taken for 1 epoch 115.51318764686584 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3111\n",
            "Epoch 50 Loss 0.2982\n",
            "Time taken for 1 epoch 116.15382766723633 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.2725\n",
            "Epoch 51 Loss 0.3044\n",
            "Time taken for 1 epoch 115.70306611061096 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.2843\n",
            "Epoch 52 Loss 0.2821\n",
            "Time taken for 1 epoch 116.45004391670227 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.2598\n",
            "Epoch 53 Loss 0.2547\n",
            "Time taken for 1 epoch 115.29516339302063 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.2394\n",
            "Epoch 54 Loss 0.2385\n",
            "Time taken for 1 epoch 115.64242839813232 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2359\n",
            "Epoch 55 Loss 0.2278\n",
            "Time taken for 1 epoch 116.29079556465149 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2132\n",
            "Epoch 56 Loss 0.2275\n",
            "Time taken for 1 epoch 114.96335172653198 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.2247\n",
            "Epoch 57 Loss 0.2333\n",
            "Time taken for 1 epoch 116.4842300415039 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2221\n",
            "Epoch 58 Loss 0.2242\n",
            "Time taken for 1 epoch 116.69811153411865 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2091\n",
            "Epoch 59 Loss 0.2167\n",
            "Time taken for 1 epoch 117.22408604621887 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.1928\n",
            "Epoch 60 Loss 0.1964\n",
            "Time taken for 1 epoch 114.7577817440033 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.1956\n",
            "Epoch 61 Loss 0.1890\n",
            "Time taken for 1 epoch 116.90420794487 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.1791\n",
            "Epoch 62 Loss 0.1719\n",
            "Time taken for 1 epoch 116.3192811012268 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.1672\n",
            "Epoch 63 Loss 0.1524\n",
            "Time taken for 1 epoch 115.1447217464447 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.1443\n",
            "Epoch 64 Loss 0.1369\n",
            "Time taken for 1 epoch 117.35539698600769 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.1333\n",
            "Epoch 65 Loss 0.1247\n",
            "Time taken for 1 epoch 116.64811038970947 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.1203\n",
            "Epoch 66 Loss 0.1135\n",
            "Time taken for 1 epoch 117.40648031234741 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1174\n",
            "Epoch 67 Loss 0.1094\n",
            "Time taken for 1 epoch 118.29931116104126 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.1097\n",
            "Epoch 68 Loss 0.1108\n",
            "Time taken for 1 epoch 116.07449388504028 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1298\n",
            "Epoch 69 Loss 0.1149\n",
            "Time taken for 1 epoch 117.44717144966125 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1162\n",
            "Epoch 70 Loss 0.1181\n",
            "Time taken for 1 epoch 116.42988157272339 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1266\n",
            "Epoch 71 Loss 0.1217\n",
            "Time taken for 1 epoch 115.99357867240906 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.1125\n",
            "Epoch 72 Loss 0.1138\n",
            "Time taken for 1 epoch 117.92361760139465 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1286\n",
            "Epoch 73 Loss 0.1132\n",
            "Time taken for 1 epoch 116.67802095413208 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1026\n",
            "Epoch 74 Loss 0.0912\n",
            "Time taken for 1 epoch 116.92250490188599 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1110\n",
            "Epoch 75 Loss 0.0835\n",
            "Time taken for 1 epoch 118.94325518608093 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0738\n",
            "Epoch 76 Loss 0.0640\n",
            "Time taken for 1 epoch 115.97968482971191 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0698\n",
            "Epoch 77 Loss 0.0550\n",
            "Time taken for 1 epoch 118.13638854026794 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0473\n",
            "Epoch 78 Loss 0.0417\n",
            "Time taken for 1 epoch 117.64502811431885 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0430\n",
            "Epoch 79 Loss 0.0353\n",
            "Time taken for 1 epoch 116.80116081237793 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0356\n",
            "Epoch 80 Loss 0.0301\n",
            "Time taken for 1 epoch 118.56444549560547 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0321\n",
            "Epoch 81 Loss 0.0275\n",
            "Time taken for 1 epoch 117.44858598709106 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0268\n",
            "Epoch 82 Loss 0.0247\n",
            "Time taken for 1 epoch 117.39027786254883 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0294\n",
            "Epoch 83 Loss 0.0239\n",
            "Time taken for 1 epoch 119.50226044654846 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0231\n",
            "Epoch 84 Loss 0.0217\n",
            "Time taken for 1 epoch 117.02596092224121 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0280\n",
            "Epoch 85 Loss 0.0215\n",
            "Time taken for 1 epoch 116.9005343914032 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0200\n",
            "Epoch 86 Loss 0.0197\n",
            "Time taken for 1 epoch 120.23295950889587 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0256\n",
            "Epoch 87 Loss 0.0197\n",
            "Time taken for 1 epoch 116.76893973350525 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0209\n",
            "Epoch 88 Loss 0.0189\n",
            "Time taken for 1 epoch 120.04856538772583 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0243\n",
            "Epoch 89 Loss 0.0186\n",
            "Time taken for 1 epoch 115.79606747627258 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0172\n",
            "Epoch 90 Loss 0.0173\n",
            "Time taken for 1 epoch 116.70239520072937 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0253\n",
            "Epoch 91 Loss 0.0179\n",
            "Time taken for 1 epoch 119.4794590473175 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0183\n",
            "Epoch 92 Loss 0.0169\n",
            "Time taken for 1 epoch 116.87405395507812 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0193\n",
            "Epoch 93 Loss 0.0166\n",
            "Time taken for 1 epoch 116.66057920455933 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0168\n",
            "Epoch 94 Loss 0.0158\n",
            "Time taken for 1 epoch 119.27819514274597 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0183\n",
            "Epoch 95 Loss 0.0154\n",
            "Time taken for 1 epoch 118.45773243904114 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0153\n",
            "Epoch 96 Loss 0.0148\n",
            "Time taken for 1 epoch 117.69162201881409 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0175\n",
            "Epoch 97 Loss 0.0147\n",
            "Time taken for 1 epoch 118.65285348892212 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0148\n",
            "Epoch 98 Loss 0.0143\n",
            "Time taken for 1 epoch 116.67162227630615 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0165\n",
            "Epoch 99 Loss 0.0143\n",
            "Time taken for 1 epoch 119.6058247089386 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0149\n",
            "Epoch 100 Loss 0.0140\n",
            "Time taken for 1 epoch 116.2376959323883 sec\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.0160\n",
            "Epoch 101 Loss 0.0140\n",
            "Time taken for 1 epoch 116.17124056816101 sec\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.0142\n",
            "Epoch 102 Loss 0.0136\n",
            "Time taken for 1 epoch 119.23223805427551 sec\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0155\n",
            "Epoch 103 Loss 0.0136\n",
            "Time taken for 1 epoch 116.57024216651917 sec\n",
            "\n",
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "410XJLATWxHO",
        "outputId": "70f80977-bca7-4176-82b4-090c74120e83"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x22b8d106ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_5ToY3PWxHO"
      },
      "source": [
        "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    # 입력 inputs의 인덱스를 단어로 디코딩해 문장을 생성함\n",
        "    # 예: [2, 1099, 1, 0, 0 ,..] => '<start> who <end>'\n",
        "    sentence = ''\n",
        "    for i in inputs[0]:\n",
        "        if i == 0:\n",
        "            break\n",
        "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
        "    \n",
        "    # 마지막에 추가된 스페이스를 지움 \n",
        "    sentence = sentence[:-1]\n",
        "    \n",
        "    #NumPy ndarray 자료형을 변환\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "    \n",
        "    # 모든 원소의 값이 0인 텐서를 생성\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    \n",
        "    # inputs를 encoder에 입력\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    \n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    # Step-by-step으로 decoder를 실행해 문장을 번역함\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # attention weight heat map를 그리기 위해 매 step의 attention 가중치 저장\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        \n",
        "        # 텐서 안에서 predictions[0]을 따라 가장 큰 값의 인덱스를 찾기\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # decoder가 예측한 인덱스를 단어로 디코딩함\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        # docoder 실행 중 '<end>'가 나타나면 문장이 끝이므로 실행 중지\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # 예측한 출력을 decoder의 다음 step의 입력으로 사용하기 위해 dimension을 expand 함\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDHk1RIjWxHO"
      },
      "source": [
        "def predict_random_val_sentence():\n",
        "    \"\"\"테스트셋에서 랜덤으로 문장을 선택해 번역하고 attention heatmap을 그리는 함수\"\"\"\n",
        "    \n",
        "    actual_sent = ''\n",
        "    \n",
        "    # 테스트셋에서 랜덤으로 인코딩 된 문장을 선택\n",
        "    k = np.random.randint(len(input_tensor_val))\n",
        "    random_input = input_tensor_val[k]\n",
        "    random_output = target_tensor_val[k]\n",
        "    random_input = np.expand_dims(random_input,0)\n",
        "    \n",
        "    # evaluate 함수를 사용해 번역 작업을 진행함\n",
        "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp,\\\n",
        "                                                max_length_targ)\n",
        "    print('Input: {}'.format(sentence[8:-6]))  # 입력 문장의 <start>와 <end>를 제거하고 출력\n",
        "    print('Predicted translation: {}'.format(result[:-6]))  # 번역한 문장에서 <end>를 제거하고 출력\n",
        "    # 랜덤으로 선택한 입력 문장의 정답 레이블을 인텍스에서 단어로 디코딩함\n",
        "    for i in random_output:\n",
        "        if i == 0:\n",
        "            break\n",
        "        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
        "    actual_sent = actual_sent[8:-7]   # 타깃 문장의 <start>와 <end>를 제거하고 출력\n",
        "    print('Actual translation: {}'.format(actual_sent))\n",
        "    \n",
        "    # attention weight \n",
        "    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
        "    \n",
        "    # 영문장 및 번역문 토큰 리스트 지정\n",
        "    sentence, result = sentence.split(' '), result.split(' ')\n",
        "    sentence = sentence[1:-1]\n",
        "    result = result[:-2]\n",
        "    \n",
        "    # plotly를 사용해 attention weight의 heat map를 생성함\n",
        "    trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='Reds')\n",
        "    data=[trace]\n",
        "    iplot(data)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20HEUws0WxHP",
        "outputId": "db3452d5-9cf3-4b15-8a9c-fe84151cc532"
      },
      "source": [
        "predict_random_val_sentence()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: i used to be poor like you\n",
            "Predicted translation: \n",
            "Actual translation: 나는 너처럼 가난했었다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "linkText": "Export to plot.ly",
                "plotlyServerURL": "https://plot.ly",
                "showLink": false
              },
              "data": [
                {
                  "colorscale": "Reds",
                  "type": "heatmap",
                  "uid": "1ae42e5c-cb2d-4bb9-ae5a-d1b7f05d8c22",
                  "x": [
                    "i",
                    "used",
                    "to",
                    "be",
                    "poor",
                    "like",
                    "you"
                  ],
                  "y": [],
                  "z": []
                }
              ],
              "layout": {}
            },
            "text/html": [
              "<div>\n",
              "        \n",
              "        \n",
              "            <div id=\"d982742a-f884-4648-970b-7741f0a284fb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                require([\"plotly\"], function(Plotly) {\n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
              "                    \n",
              "                if (document.getElementById(\"d982742a-f884-4648-970b-7741f0a284fb\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd982742a-f884-4648-970b-7741f0a284fb',\n",
              "                        [{\"colorscale\": \"Reds\", \"type\": \"heatmap\", \"uid\": \"1ae42e5c-cb2d-4bb9-ae5a-d1b7f05d8c22\", \"x\": [\"i\", \"used\", \"to\", \"be\", \"poor\", \"like\", \"you\"], \"y\": [], \"z\": []}],\n",
              "                        {},\n",
              "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d982742a-f884-4648-970b-7741f0a284fb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                });\n",
              "            </script>\n",
              "        </div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0xHwA3qWxHP"
      },
      "source": [
        ""
      ],
      "execution_count": NULL,
      "outputs": []
    }
  ]
}
